# -*- coding: utf-8 -*-
"""ProjectNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pd6jYPjxeD9P0Bt1eX2szQQ4EIb4Mm_b
"""

import numpy as np
import pandas as pd
import nltk
import seaborn as sns
import matplotlib.pyplot as plt
import re

df=pd.read_csv('/content/twitter_validation.csv',header=None,encoding='ISO-8859-1')#to read symbols and special characters
df.columns=['id','Social_media','Target','Text']
df

df.head()

df.tail()

df.isna().sum()

df.dtypes

df['Target'].value_counts()

sns.countplot(x=df['Target'],color='red')

df['Social_media'].value_counts()

sns.countplot(df['Social_media'],color='purple')

df['Target'].unique()

df.drop(df.index[(df['Target']=='Irrelevant')],axis=0,inplace=True)
df['Target'].value_counts()
df

df.reset_index(drop=True,inplace=True)
df

df.drop(['id','Social_media'],axis=1,inplace=True)
df

#positive :1 negative:-1 neutral:0
df['Target']=df['Target'].str.replace('Positive','1')
df['Target']=df['Target'].str.replace('Negative','-1')
df['Target']=df['Target'].str.replace('Neutral','0')
df
#df['target']=df['target].map({'Positive':1,'Nuetral':0,'Negative':-1})

df.dtypes

df['Target']=df['Target'].astype(int)
df.dtypes

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

tweets=df.Text
tweets

#Tokenization
from nltk import TweetTokenizer
tk=TweetTokenizer()
tweets=tweets.apply(lambda x:tk.tokenize(x)).apply(lambda x:" ".join(x))
tweets

#removing special characters
tweets=tweets.str.replace('[^a-zA-Z0-9]+',' ')
tweets

#do ws #len take only words which has len greater than three
from nltk.tokenize import word_tokenize
tweets=tweets.apply(lambda x:' '.join([w for w in word_tokenize(x) if len(w)>=3]))
tweets

#stemming
from nltk.stem import SnowballStemmer
stemmer=SnowballStemmer('english')
tweets=tweets.apply(lambda x:[stemmer.stem(i.lower()) for i in tk.tokenize(x)]).apply(lambda x:' '.join(x))
tweets

#removing stopwords
from nltk.corpus import stopwords
nltk.download('stopwords')
sw=stopwords.words('english')
tweets=tweets.apply(lambda x:[i for i in tk.tokenize(x) if i not in sw]).apply(lambda x:' '.join(x))
tweets

from sklearn.feature_extraction.text import TfidfVectorizer
vec=TfidfVectorizer()
train_data=vec.fit_transform(tweets)

train_data.shape

print(train_data)

y=df['Target'].values
y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(train_data,y,test_size=0.30,random_state=42)
x_train

x_test

y_train

y_test

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,accuracy_score
from sklearn.metrics import classification_report
k_model=KNeighborsClassifier(n_neighbors=7)
n_model=MultinomialNB()
s_model=SVC()
lst_model=[k_model,n_model,s_model]

for i in lst_model:
  print("model name is:",i)
  i.fit(x_train,y_train)
  y_pred=i.predict(x_test)
  print('**********************************************')
  print(confusion_matrix(y_test,y_pred))
  print('Accuracy score.........................')
  print(accuracy_score(y_test,y_pred))
  print('Classification.........................')
  print(classification_report(y_test,y_pred))